\chapter{Dynamic Memory Management}
\label{chp:dynamic-memory-management}
In order to allow objects to live outside of static scopes, we need to utilize a different memory management technique, such that objects are not allocated on the stack. Dynamic memory management presents a method of storing objects in different memory structures, most commonly, a memory heap. Most irreversibly, modern programming languages uses dynamic memory management in one some form for allocating space for objects in memory. 

However, reversible, native support of complex data structures is a non-trivial matter to implement. Variable-sized records and frames need to be stored efficiently in a structured heap, while avoiding garbage build-up to maintain reversibility. A reversible heap manager layout has been proposed for a simplified version of the reversible functional language \textsc{RFun} and later expanded to allow references to avoid deep copying values~\cite{ha:heap, ty:rfun, tm:refcounting}.

This chapter presents a brief introduction to fragmentation, garbage and linearity and how these respectively are handled reversibly, and a discussion of various heap manager layouts considered for \rooplpp, along with their advantages and disadvantages in terms of implementation difficulty, garbage build-up and the OOP domain. 


\section{Fragmentation}
\label{sec:fragmentation}
Efficient memory usage is an important matter to consider when designing a heap layout for a dynamic memory manager. In a stack allocating memory layout, the stack discipline is in effect, meaning only the most recently allocated data can be freed. This is not the case with heap allocation, where data can be freed regardless of allocation order. A potential side effect of this freedom, comes as a consequence of memory fragmentation. We distinguish different types of fragmentation as internal or external fragmentation. Internal fragmentation refers to unused space inside a memory block used to store an object, if, say, the object is smaller than the block it has been allocated to. External fragmentation occurs as blocks freed throughout execution are spread across the memory heap, resulting in \textit{fragmented} free space~\cite{tm:languages}. 

\subsection{Internal Fragmentation}
\label{subsec:internal-fragmentation}
Internal fragmentation occurs in the memory heap when part of an allocated memory block is unused. This type of fragmentation can arise from a number of different scenarios, but mostly it originates from cases of \textit{over-allocation}, which occurs when the memory manager delegates memory larger than required to fit an object, due to e.g. fixed-block sizing. 

For an example, consider a scenario, in which we are allocating memory for an object of size $m$ onto a simple, fixed-sized block heap. The fixed block size is $n$ and $m \neq n$. If $n > m$, internal fragmentation would occur of size $n-m$ for every object of size $m$ allocated in said heap. If $n < m$, numerous blocks would be required for allocation to fit our object. In this case the internal fragmentation would be of size $n - m\ mod\ n$ per allocated object of size $m$.

\begin{subfigures}
  \begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      % Background
      \fill[fill = grey] (0, 0) rectangle (2, 1) node[midway] {};
  
      % Frame
      \draw (0,0) rectangle (3,1);
      \draw (3,0) rectangle (6,1);
      \draw[dashed] (2, 0) -- (2, 1); 
      
      \draw (1, 0.5) node{Object};
      \draw (4.5, 0.5) node{Free space};
  
      %braces
      \draw[decoration={brace,mirror,raise=5pt},decorate] (0,0) -- node[below=6pt] {$n$} (3,0);
      \draw[decoration={brace,raise=5pt},decorate] (0,1) -- node[above=6pt] {$m$} (2,1);
      \draw[decoration={brace,raise=5pt},decorate] (2,1) -- node[above=6pt] {$n-m$} (3,1); 
    \end{tikzpicture}
    \caption{Creation of internal fragmentation of size $n-m$ due to \textit{over-allocation}}
    \label{fig:internal-frag-example}
  \end{figure}

  \begin{figure}[ht]
    \centering
    \begin{tikzpicture}
      % Background
      \fill[fill = grey] (0, 0) rectangle (4, 1) node[midway] {};
  
      % Frame
      \draw (0,0) rectangle (6,1);
      \draw (6,0) rectangle (9,1);
      \draw[dashed] (4, 0) -- (4, 1);
      
      \draw (2.1, 0.5) node{Object};
      \draw (7.5, 0.5) node{Free space};
  
      %braces
      \draw[decoration={brace,mirror,raise=5pt},decorate] (0,0) -- node[below=6pt] {$n$} (3,0);
      \draw[decoration={brace,raise=5pt},decorate] (0,1) -- node[above=6pt] {$m$} (4,1);
      \draw[decoration={brace,mirror,raise=5pt},decorate] (4,0) -- node[below=6pt] {$n-m\ mod\ n$} (6,0); 
    \end{tikzpicture}
    \caption{Creation of internal fragmentation of size $n-m\ mod\ n$ due to \textit{over-allocation}}
    \label{fig:internal-frag-example-cont}
  \end{figure}
\end{subfigures}

Figure~\ref{fig:internal-frag-example} and~\ref{fig:internal-frag-example-cont} visualizes the examples of internal fragmentation build-up from \textit{over-allocating} memory. 

It is difficult for the memory manager to reclaim wasted memory caused by internal fragmentation, as it usually originates from a design choice.
Intuitively, internal fragmentation can best be prevented by ensuring that the size of the block(s) being used for allocating space for an object of size $m$ either match or sums to this exact size, when designing the layout. 

\subsection{External Fragmentation}
\label{subsec:external-fragmentation}
External fragmentation materializes in the memory heap when a freed block becomes partly or completely unusable for future allocation if it, say, is surrounded by allocated blocks but the size of the freed block is too small to contain objects on its own.

This type of fragmentation is generally a more substantial cause of problems than internal fragmentation, as the amount of wasted memory typically is larger and less predictable in external fragmentation blocks than in internal fragmentation blocks. Depending on the heap implementation, i.e. a layout using variable-sized blocks of, say, size $2^n$, the internal fragment size becomes considerable for large $n$s. 

Non-allocatable external fragments become a problem when it is impossible to allocate space for a large object as a result of too many non-consecutive blocks scattered around the heap, caused by the external fragmentation. Physically, there is enough space to store the object, but not in the current heap state. In this scenario we would need to relocate blocks in such a manner that the fragmentation disperses, which is not possible to do reversibly.

Allocation and deallocation order is important in order to combat external fragmentation. For example, if we have a class $A$, which fit on one memory block of size $n$, and we have a class $B$, which fit on two memory blocks of size $n$ and limited memory space, we can easily reach a situation, where we cannot fit more $B$ objects due to external fragmentation.

\begin{figure}[ht]
  \centering
  \begin{subfigure}{.4\textwidth}
    \centering
    \begin{tikzpicture}
      % Frame
      \draw (0, 0) rectangle (1, 1);
      \draw (1, 0) rectangle (2, 1);
      \draw (2, 0) rectangle (3, 1);
      \draw (3, 0) rectangle (4, 1);

      % Brace
      \draw[decoration={brace,raise=5pt},decorate] (0,1) -- node[above=6pt] {$n$} (1,1);
    \end{tikzpicture}
    \caption{\footnotesize Free memory of size $4n$}
  \end{subfigure}
  \begin{subfigure}{.4\textwidth}
    \centering
    \begin{tikzpicture}
      % Background
      \fill[fill = grey] (0, 0) rectangle (3, 1) node[midway] {};
  
      % Frame
      \draw (0, 0) rectangle (1, 1) node[pos=.5] {$A$};
      \draw (1, 0) rectangle (3, 1) node[pos=.5] {$B$};
      \draw (3, 0) rectangle (4, 1);

      % Braces
      \draw[decoration={brace,raise=5pt},decorate] (0,1) -- node[above=6pt] {$n$} (1,1);
      \draw[decoration={brace,raise=5pt},decorate] (1,1) -- node[above=6pt] {$2n$} (3,1);
    \end{tikzpicture}
    \caption{\footnotesize Allocate $A$ and $B$}
  \end{subfigure}
  \vskip 1em
  \begin{subfigure}{.6\textwidth}
    \centering
    \begin{tikzpicture}
      % Background
      \fill[fill = grey] (1, 0) rectangle (3, 1) node[midway] {};
  
      % Frame
      \draw (0, 0) rectangle (1, 1);
      \draw (1, 0) rectangle (3, 1) node[pos=.5] {$B$};
      \draw (3, 0) rectangle (4, 1);
    \end{tikzpicture}
    \caption{\footnotesize Free $A$. Cannot fit another $B$ due to external fragmentation}
  \end{subfigure}
  \caption{Example of external fragmentation caused for allocation and deallocation order}
  \label{fig:external-frag-example}
\end{figure}  

Figure~\ref{fig:external-frag-example} shows this example, where the allocation and deallocation order causes a situation, in which we cannot allocate any more $B$ objects, even though we physically have the required amount of free space in memory. 

\begin{figure}[ht]
  \centering
  \begin{subfigure}{.4\textwidth}
    \centering
    \begin{tikzpicture}
      % Frame
      \draw (0, 0) rectangle (1, 1);
      \draw (1, 0) rectangle (2, 1);
      \draw (2, 0) rectangle (3, 1);
      \draw (3, 0) rectangle (4, 1);

      % Brace
      \draw[decoration={brace,raise=5pt},decorate] (0,1) -- node[above=6pt] {$n$} (1,1);
    \end{tikzpicture}
    \caption{\footnotesize Free memory of size $4n$}
  \end{subfigure}
  \begin{subfigure}{.4\textwidth}
    \centering
    \begin{tikzpicture}
      % Background
      \fill[fill = grey] (0, 0) rectangle (3, 1) node[midway] {};
  
      % Frame
      \draw (0, 0) rectangle (2, 1) node[pos=.5] {$B$};
      \draw (2, 0) rectangle (3, 1) node[pos=.5] {$A$};
      \draw (3, 0) rectangle (4, 1);

      % Braces
      \draw[decoration={brace,raise=5pt},decorate] (0,1) -- node[above=6pt] {$2n$} (2,1);
      \draw[decoration={brace,raise=5pt},decorate] (2,1) -- node[above=6pt] {$n$} (3,1);
    \end{tikzpicture}
    \caption{\footnotesize Allocate $B$ and $A$}
  \end{subfigure}
  \vskip 1em
  \begin{subfigure}{.4\textwidth}
    \centering
    \begin{tikzpicture}
      % Background
      \fill[fill = grey] (0, 0) rectangle (2, 1) node[midway] {};
  
      % Frame
      \draw (0, 0) rectangle (2, 1) node[pos=.5] {$B$};
      \draw (2, 0) rectangle (3, 1);
      \draw (3, 0) rectangle (4, 1);
    \end{tikzpicture}
    \caption{\footnotesize Free $A$}
  \end{subfigure}
  \begin{subfigure}{.4\textwidth}
    \centering
    \begin{tikzpicture}
      % Background
      \fill[fill = grey] (0, 0) rectangle (4, 1) node[midway] {};
  
      % Frame
      \draw (0, 0) rectangle (2, 1) node[pos=.5] {$B$};
      \draw (2, 0) rectangle (4, 1) node[pos=.5] {$B$};
    \end{tikzpicture}
    \caption{\footnotesize Allocate another $B$}
  \end{subfigure}
  \caption{Example of avoiding external fragmentation using allocation and deallocation order}
  \label{fig:external-frag-example-cont}
\end{figure}  

Figure~\ref{fig:external-frag-example-cont} shows how changing allocation and deallocation order can combat external fragmentation.


\section{Memory Garbage}
\label{sec:memory-garbage}
A reversible computation should be garbage-free and as such it should be our goal to return the memory to its original state after program termination.

Traditionally, in non-reversible programming languages, freed memory blocks are simply re-added to the free-list during deallocation and no modification of the actual data stored in the block is performed, as it simply is overwritten when the block is used later on. In the reversible setting we must return the memory block to its original state after the block has been freed (e.g. zero-cleared), to uphold the time-invertible and two-directional computational model. Figure~\ref{fig:injective-garbage-in-out} illustrates how the output data (or garbage) of an injective function $f$ is the input to its inverse function $f'$.

In heap allocation layouts, we maintain one or more free-lists to keep track of free blocks during program execution, which are stored in memory, besides the heap representation itself. These free-lists can essentially be considered garbage and as such, they must also be returned to their original state after execution. Furthermore, the heap itself can also be considered garbage and if it grows during execution, it should also be returned to its original size.

\begin{figure}[ht]
  \centering
    \begin{tikzpicture}
      % lines
      \draw[-] (-1,1.75) node[left]{$in$} -- (9,1.75) node[right] {$in$};
      \draw[-] (2,1.25) -- (6,1.25);
      \draw[-] (2,0.75) -- (6,0.75);
      \draw[-] (2,0.25) -- (6,0.25);
      \draw[-] (-1,-0.5) node[left]{$out$} -- (9,-0.5) node[right] {$out$};
      \draw[-] (4,0.24) node[circle,fill,inner sep=1pt] {} -- (4,-0.5) node[circle,fill,inner sep=1pt] {};
      
      % boxes
      \draw[fill = white] (0, 0) rectangle (2, 2) node[pos=0.5] {\Large $f$};
      \draw[fill = white] (6, 0) rectangle (8, 2) node[pos=0.5] {\Large $f'$};
      \node[diamond, fill=white, draw] at (4,1.25) {\scriptsize $garbage$};
    \end{tikzpicture}
    \caption{The "garbage" output of an injective function $f$ is the input to its inverse function $f'$}
    \label{fig:injective-garbage-in-out}
\end{figure}

Returning the free-list(s) to their original states is a non-trivial matter, which is highly dependent on the heap layout and free-list design.~\citeauthor{ha:heap} introduced a dynamic memory manager which allowed heap allocation and deallocation, but without restoring the free-list to its original state in~\cite{ha:heap}.~\citeauthor{ha:heap} argue that an unrestored free-list can be considered harmless garbage in the sense that the free-list residing in memory after termination is equivalent to a restored free-list, as it contains the same blocks, but linked in a different order, depending on the order of allocation and deallocation operations performed during program execution. Figure~\ref{fig:equivalent-free-lists} illustrates how an inverse, injective function $f'$, whose non-inverse function $f$ computes something which modifies a given free lists, does not require the \textit{exact} output free list of $f$, but \textit{any} free list of same layout as input for the inverse function $f'$. The output free list of $f'$ will naturally be a further modified free list.

\begin{figure}[ht]
  \centering
  % \includegraphics[width=0.7\textwidth]{garbage-classes}
  \begin{tikzpicture}
    % lines
    \draw[-] (-1,1.75) node[left]{$free\ list$} -- (2,1.75);
    \draw[-] (-1,0.25) node[left]{$in$} -- node[below] {$out$} (10,0.25) node[right] {$in$};
    \draw (7,1.75) .. controls (6,1.75) and (6,1.75) .. (6,2.75) node[above] {$\forall\ free\ lists$};
    \draw (2,1.75) .. controls (3,1.75) and (3,1.75) .. (3,2.75) node[above] {$free\ list'$};
    \draw[-] (9,1.75) -- (10,1.75) node[right]{$free\ list''$};
    
    % boxes
    \draw[fill = white] (0, 0) rectangle (2, 2) node[pos=0.5] {\Large $f$};
    \draw[fill = white] (7, 0) rectangle (9, 2) node[pos=0.5] {\Large $f'$};
  \end{tikzpicture}
  \caption{All free-lists are considered equivalent "garbage" in terms of injective functions}
  \label{fig:equivalent-free-lists}
\end{figure}

This intuitively leads to the question of garbage classification. In the reversible setting all functions are injective. Thus, given some $input_f$, in a reversible computation using heap allocation, the injective function $f$ produces some $output_f$ and some $garbage_f$ (e.g. garbage in form of storing data in the heap, so the free list changes, the heap grows, etc.). Its inverse function $f^{-1}$ must thus take $f$'s $output_f$ and $garbage_f$ as $input_{f^{-1}}$ to produce its output $output_{f^{-1}}$ which is $f$'s $input_f$. However, in the context of reversible heaps, we must consider all free-lists as of "equivalent garbage class" and thus freely substitutable with each other, as injective functions still can drastically change the block layout, free-list order, etc. during its execution in either direction. Figure~\ref{fig:equivalent-free-lists} shows how any free-list can be passed between a function $f$ and its inverse $f^{-1}$.


\section{Linearity and Reference Counting}
\label{sec:referencing}
% cite: http://home.pipeline.com/~hbaker1/Use1Var.html
Programming languages uses different approaches for storing and synchronizing variables and objects in memory. Typing \textit{linearity} is a distinction, which can reduce storage management and synchronization costs. %TODO: Cite here.

Reversible programming languages such as \textsc{Janus} and \textsc{Roopl} are linear in the sense that object and variable pointers cannot be copied and are only deleted during deallocation. Pointer copying greatly increases the flexibility of programming, especially in a reversible settings where zero-clearing is critical, at the cost of increased management in form of reference counting for e.g. objects. For variables, pointer copying is not particular interesting, nor would it add much flexibility as the values of a variable simply can be copied into statically-scoped local blocks. For objects however, tedious amounts of boilerplate work must be done if object $A$ and $B$ need to work on the same object $C$ and only one reference to each object is allowed.

 \citeauthor{tm:refcounting} presented the reversible functional language \textsc{Rcfun} which used reference counting to allow multiple pointers to the same memory nodes as well as a translation from \textsc{Rcfun} into \textsc{Janus} in \cite{tm:refcounting}. In \textsc{Rcfun}, reference counting is used to manage and trace the number of pointer copies made by respectively incrementing and decrementing a \textit{reference count} stored in the memory node, whenever the original node pointer is copied or a copy pointer is deleted. For the presented heap manage, deletion of object nodes was only allowed when no references to a node remained.

In non-reversible languages, reference counting is also used in garbage collection by automatically deallocating unreachable objects and variables which contains no referencing. 


\section{Heap Manager Layouts}
\label{sec:heap-manager-layout}
Heap managers can be implemented in numerous ways. Different layouts yield advantages when allocating memory, finding a free block or when collecting garbage. As our goal is to construct a garbage-free heap manager, our finalized design should emphasize and reflect this objective in particular. Furthermore, we should attempt to allocate and deallocate memory as efficiently as possible, as merging and splitting of blocks is a non-trivial problem in a reversible setting and to avoid problematic fragmentation.

For the sake of simplicity, we will not consider the the issue of retrieving memory pages reversibly. A reversible operating system is a long-term dream of the reversible researcher and as reversible programming language designers, we assume that \rooplpp will be running in an environment, in which an operating system will be supplying memory pages and their mappings. As such, the following heap memory designs reflect this preliminary assumption, that we always can query the operating system for more memory. 

Historically, most object-oriented programming languages utilize a dynamic memory manager during program execution. In older, lower-level languages such as \textsc{C}, memory management is manual and allocation has to be stated explicitly and with the requested size through the \texttt{malloc} statement and deallocated using the \textbf{free} statement. Modern languages, such as \textsc{C\texttt{++}}, \textsc{Java} and \textsc{Python}, \textit{automagically} allocates and frees space for objects and variable-sized arrays by utilizing their dynamic memory manager and garbage collector to dispatch \textbf{malloc}- and \textbf{free}-like operations to the operating system and managing the obtained memory blocks in private heap(s)~\cite{wh:cpp_memory, bv:jvm, py:memory}. The heap layout of these managers vary from language to language and compiler to compiler.

Previous work on reversible heap manipulation has been done for reversible functional languages in~\cite{ha:heap, jsk:translation, tm:garbage}.

\citeauthor{ha:heap} presented a static heap structure consisting of \textsc{Lisp}-inspired constructor cells of fixed size and a single free list for the reversible function language \textsc{Rfun} in~\cite{ha:heap}. \citeauthor{tm:refcounting} presented an implementation in \textsc{Janus} of reversible reference counting under the assumption of \citeauthor{ha:heap}'s heap manager in~\cite{tm:refcounting}. Building on the previous work, \citeauthor{tm:garbage} later presented a reversible intermediate language \textsc{Ril} and an implementation in \textsc{Ril} of a reversible heap manager, which uses reference counting and hash-consing to achieve garbage collection in~\cite{tm:garbage}.

We do not consider reference counting or garbage collection in the layouts presented in the following sections, but we later show how the selected layout for \rooplpp is extended with reference counting in~\ref{sec:referencing-compilation}.

\subsection{Memory Pools}
\label{subsec:memory-pools}
The simplest heap layout we can design simple uses fixed-sized blocks. This design is also known as memory pools, as memory is allocated from "pools" of fixed-sized blocks regardless of the record size.
To model these pools of fixed-sized blocks, we simply use a linked-list of identically sized free block cells, which we maintain over execution.
While the fixed-block layout is simple and relatively easy in terms of implementation it is also largely uninteresting as it provides little to no options, besides sizing of the fixed-blocks, to combat fragmentation.

This layout comes with a few options in terms of the actual heap layout. If we only allow allocation of consecutive, adjacent free blocks, we should keep the free list sorted. If the free list is not sorted, and we have to allocate an object which requires $n$ blocks, we have to iterate the free list $n^2$ times in the worst case to find a chain of consecutive blocks large enough to fit the object. The sorting part itself is non-trivial matter. Furthermore, we need some overhead storage inside the object to contains the references of the blocks occupied by the object, or some other structure which can be used when deallocating the object and returning all the blocks to the free list. If we allow allocation of non-consecutive blocks, larger amounts of bookkeeping is required as we need to store knowledge of when and where the object is split.

Figures~\ref{fig:external-frag-example} and~\ref{fig:external-frag-example} from earlier in this chapter, in section~\ref{subsec:external-fragmentation} on page~\pageref{fig:external-frag-example} illustrates examples with consecutive, fixed-sized block allocation. 


\subsection{One Heap Per Record Size}
\label{subsec:one-heap-per-record-size}
Instead of allocating space for objects from a single free list and heap, we could design an approach which uses one heap per record size. The respective classes and their sizes are easily identified during compile time from which the amount of heaps and free list will be initialized. This means the layout is very dynamic and potentially can change drastically in terms of the amount of heaps utilized depending on the input program. 

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    % Heaps
    \draw[step=1cm] (0,4) grid (4,5);
    \node[right] at (4,4.5) {$A$ heap}; 
    \draw (0,2) rectangle (2,3) rectangle node[right, xshift=1cm] {$B$ heap} (4,2);
    \draw (0,0) rectangle node[right, xshift=2cm] {$C$ heap} (4,1);

    % Free lists
    \draw (-2.5, 4) rectangle node[left, xshift=-0.5cm] {$A$ free list} (-1.5, 5);
    \draw (-2.5, 2) rectangle node[left, xshift=-0.5cm] {$B$ free list} (-1.5, 3);
    \draw (-2.5, 0) rectangle node[left, xshift=-0.5cm] {$C$ free list} (-1.5, 1);
    
    % Arrow for 1st heap pair
    \node[circle,fill,inner sep=1pt] at (-2, 4.5) {};
    \draw[-latex] (-2, 4.5) to[out=45, in=135] (0.25, 4.5);
    \node[circle,fill,inner sep=1pt] at (0.75, 4.5) {};
    \draw[-latex] (0.75, 4.5) to[out=45, in=135] (1.25, 4.5);
    \node[circle,fill,inner sep=1pt] at (1.75, 4.5) {};
    \draw[-latex] (1.75, 4.5) to[out=45, in=135] (2.25, 4.5);
    \node[circle,fill,inner sep=1pt] at (2.75, 4.5) {};
    \draw[-latex] (2.75, 4.5) to[out=45, in=135] (3.25, 4.5);

    % Arrow for 2nd heap pair
    \node[circle,fill,inner sep=1pt] at (-2, 2.5) {};
    \draw[-latex] (-2, 2.5) to[out=45, in=135] (0.5, 2.5);
    \node[circle,fill,inner sep=1pt] at (1.5, 2.5) {};
    \draw[-latex] (1.5, 2.5) to[out=45, in=135] (2.5, 2.5);

    % Arrow for 3rd heap pair
    \node[circle,fill,inner sep=1pt] at (-2, 0.5) {};
    \draw[-latex] (-2, 0.5) to[out=45, in=135] (0.5, 0.5);

    % Braces
    \draw[decoration={brace,raise=5pt},decorate] (0,1) -- node[above=6pt] {$4n$} (4,1);
    \draw[decoration={brace,raise=5pt},decorate] (0,3) -- node[above=6pt] {$2n$} (2,3);
    \draw[decoration={brace,raise=5pt},decorate] (0,5) -- node[above=6pt] {$n$} (1,5);
  \end{tikzpicture}
  \caption{Memory layout using one heap per record size}
  \label{fig:one-heap-per-record-size}
\end{figure}

Figure~\ref{fig:one-heap-per-record-size} illustrates three heaps with respective free lists for three classes $A$, $B$ and $C$ of size $n$, $2n$ and $4n$. Each heap is represented as a simple linked list with the free list simply being a pointer to the first free block in the heap. 

The advantage of this approach would be effective elimination of internal and external fragmentation, as each heap fits their targeted record perfectly, making each allocation and deallocation tailored to the size of the record obtained from a static analysis during compilation, resulting in no over-allocation and no unusable chunks of freed memory appearing during varying deallocation order. Implementation-wise, allocation of an object of a given class simply becomes the task of popping the head of the respective free list, which easily can be determined at compile time. The inverse deallocation is simply added a new head to the free list. 

The obvious disadvantage is the amount of book-keeping and workload associated with growing and shrinking a heap and its neighbours, in case the program requests additional memory from the operating system. In real world object-oriented programming, most classes features a small number of fields, very rarely more than 16. As such, multiple heaps of same record size would exist, which intuitively seems inefficient. 

Additional, small helper classes would spawn additional heaps and additional book-work, making the encapsulation concept of OOP rather unattractive, for the optimization-oriented reversible programmer. 

Finally, while internal and external fragmentation is effectively eliminated, we are left with additional and considerable amounts of garbage in forms of all the heaps and free lists initialized in memory. If two record types only differ one word in size, two heaps would be initialized. Each heap intuitively needs to be initialized with a chunk of memory from the underlying operating system such that objects can be allocated on their respective heaps, regardless of the number of times the heap is used during program execution. This is an obvious space requirement increase over the previously presented layout, and on average, the amount of required memory for a program compiled using this approach would probably be larger, than some of the following layouts, due to unoptimized heap utilization and sharing.


\subsection{One Heap Per Power-Of-Two}
\label{subsec:one-heap-per-power-of-two}
% TODO: Revise
A different approach as to having one heap per record type, would be having one heap per power-of-two until some arbitrary size. Using this approach, records would be stored in the heap which has a block size of a power-of-two closes matching to the record's size. This layout is a distinction from the "one heap per record type" as it still retains the size-optimized storing idiom but allows the heaps to contain records of mixed types. For programs with a large amount of small, simple classes needed to model some system, where each class is roughly the same size, the amount of heaps constructed would be substantially smaller than using one heap per record type, as many records will fit within the same heap. Implementation wise, the number of heaps can be determined at compile time. Furthermore, to ensure we do not end up with heaps of very large memory blocks, an arbitrary power-of-two size limit could be set at, say, one kilobyte . If any record exceeds said limit, it could be split into $\sqrt{n}$ size chunks and stored in their respective heaps.\\
This approach does, however, also suffer from large amount of book-keeping and fiddling when shrinking and growing adjacent heaps.

\texttt{Algorithm: Similar to buddy memory?}

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    % Heaps
    \draw[step=1cm] (0,4) grid (4,5);
    \node[right] at (4,4.5) {$n^1$ heap}; 
    \draw (0,2) rectangle (2,3) rectangle node[right, xshift=1cm] {$n^2$ heap} (4,2);
    \draw (0,0) rectangle node[right, xshift=2cm] {$n^3$ heap} (4,1);

    \draw (0,-3) -- (0, -2);
    \draw (0,-3) -- (1.5, -3);
    \draw (0,-2) -- (1.5, -2);
    \draw (2.5,-3) -- (4, -3);
    \draw (2.5,-2) -- (4, -2);
    \draw (4,-3) -- (4, -2);
    \node[right] at (4,-2.5) {$n^m$ heap};  

    % Free lists
    \draw (-2.5, 4) rectangle node[left, xshift=-0.5cm] {$n^1$ free list} (-1.5, 5);
    \draw (-2.5, 2) rectangle node[left, xshift=-0.5cm] {$n^2$ free list} (-1.5, 3);
    \draw (-2.5, 0) rectangle node[left, xshift=-0.5cm] {$n^3$ free list} (-1.5, 1);
    \draw (-2.5, -3) rectangle node[left, xshift=-0.5cm] {$n^m$ free list} (-1.5, -2);
    
    % Arrow for 1st heap pair
    \node[circle,fill,inner sep=1pt] at (-2, 4.5) {};
    \draw[-latex] (-2, 4.5) to[out=45, in=135] (0.25, 4.5);
    \node[circle,fill,inner sep=1pt] at (0.75, 4.5) {};
    \draw[-latex] (0.75, 4.5) to[out=45, in=135] (1.25, 4.5);
    \node[circle,fill,inner sep=1pt] at (1.75, 4.5) {};
    \draw[-latex] (1.75, 4.5) to[out=45, in=135] (2.25, 4.5);
    \node[circle,fill,inner sep=1pt] at (2.75, 4.5) {};
    \draw[-latex] (2.75, 4.5) to[out=45, in=135] (3.25, 4.5);

    % Arrow for 2nd heap pair
    \node[circle,fill,inner sep=1pt] at (-2, 2.5) {};
    \draw[-latex] (-2, 2.5) to[out=45, in=135] (0.5, 2.5);
    \node[circle,fill,inner sep=1pt] at (1.5, 2.5) {};
    \draw[-latex] (1.5, 2.5) to[out=45, in=135] (2.5, 2.5);

    % Arrow for 3rd heap pair
    \node[circle,fill,inner sep=1pt] at (-2, 0.5) {};
    \draw[-latex] (-2, 0.5) to[out=45, in=135] (0.5, 0.5);

    % Arrow for 4th heap pair
    \node[circle,fill,inner sep=1pt] at (-2, -2.5) {};
    \draw[-latex] (-2, -2.5) to[out=45, in=135] (0.5, -2.5);

    % Braces
    \draw[decoration={brace,raise=5pt},decorate] (0,5) -- node[above=6pt] {$n^1$} (1,5);
    \draw[decoration={brace,raise=5pt},decorate] (0,3) -- node[above=6pt] {$n^2$} (2,3);
    \draw[decoration={brace,raise=5pt},decorate] (0,1) -- node[above=6pt] {$n^3$} (4,1);
    \draw[decoration={brace,raise=5pt},decorate] (0,-2) -- node[above=6pt] {$n^m$} (4,-2);

    % Dots
    \node[above] at (-2, -1) {$\vdots$};
    \node[above] at (2, -1) {$\vdots$};
    \node[] at (2, -2) {$\dots$};
    \node[] at (2, -3) {$\dots$};
  \end{tikzpicture}
  \caption{Memory layout using one heap per power-of-two}
  \label{fig:one-heap-per-power-of-two}
\end{figure}

\subsection{Shared Heap, Record Size-Specific Free Lists}
\label{subsec:shared-heap}
% TODO: Revise
A natural proposal, considering the disadvantages of the previously presented design, would be using a shared heap instead of record-specific heaps. 
This way, we ensure minimal fragmentation when allocating and freeing as the different free lists ensures that allocation of an object wastes as little memory as possible. By only keeping one heap, we eliminate the growth/shrinking issues of the multiple heap layout. 

There is, however, still a considerable amount of book-keeping involved in maintaining multiple free-lists. The bigger the number of unique classes defined in a program, the more free-lists we need to maintain during execution. If the free-lists are not allowed to point at the same free block (which they intuitively shouldn't in order to ensure reversibility), a program with, say one hundred different classes of size 2, would require a hundred identical free lists. 

% TODO: Algorithm: Find block size for record type-specific free list, call get\_free on shared heap
\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    % Heap
    \draw (0, 0) rectangle (4, 1);
    \draw (4, 0) rectangle (6, 1);
    \draw (6, 0) rectangle (10, 1);
    \draw (10, 0) rectangle (11, 1);
    \draw (11, 0) rectangle (13, 1);

    % Free lists
    \draw (2, 2) rectangle (3, 3) node[midway, above, yshift=0.6cm] {$A$ free list}; 
    \draw (6, 2) rectangle (7, 3) node[midway, above, yshift=0.6cm] {$B$ free list};
    \draw (10, 2) rectangle (11, 3) node[midway, above, yshift=0.6cm] {$C$ free list};

    % Arrows 1st heap
    \node[circle,fill,inner sep=1pt] at (2.5, 2.5) {};
    \draw[-latex] (2.5, 2.5) to[out=270, in=90] (0.5, 0.5);

    \node[circle,fill,inner sep=1pt] at (3.5, 0.5) {};
    \draw[-latex] (3.5, 0.5) to[out=90, in=90] (6.5, 0.5);

    % Arrows 2nd heap
    \node[circle,fill,inner sep=1pt] at (6.5, 2.5) {};
    \draw[-latex] (6.5, 2.5) to[out=270, in=90] (4.5, 0.5);

    \node[circle,fill,inner sep=1pt] at (5.5, 0.5) {};
    \draw[-latex] (5.5, 0.5) to[out=-90, in=-90] (11.5, 0.5);

    % Arrows 3rd heap
    \node[circle,fill,inner sep=1pt] at (10.5, 2.5) {};
    \draw[-latex] (10.5, 2.5) to[out=270, in=90] (10.5, 0.5);

    % Braces
    \draw[decoration={brace, mirror, raise=5pt},decorate] (0,0) -- node[below=6pt] {$4n$} (4,0);
    \draw[decoration={brace, mirror, raise=5pt},decorate] (10,0) -- node[below=6pt] {$n$} (11,0);
    \draw[decoration={brace, raise=5pt},decorate] (11,1) -- node[above=6pt] {$2n$} (13,1);
  \end{tikzpicture}
  \caption{Record size-specific free lists on a shared heap}
  \label{fig:shared-heap}
\end{figure}

\subsection{Buddy Memory}
\label{subsec:buddy-memory}
The Buddy Memory layout utilizes blocks of variable-sizes of the power-of-two, typically with one free list per power-of-two using a shared heap. When allocating an object of size $m$, we simply check the free lists for a free block of size $n$, where $n \geq m$. Is such a block found and if $n > m$, we split the block into two halves recursively, until we obtain the smallest block capable of storing $m$. When deallocating a block of size $m$, do the action described above in reverse, thus merging the blocks again, where possible.

This layout is somewhat of a middle ground between the previous three designs, addressing a number of problems found in these. The Buddy Memory layout uses a single heap for all record-types, thus eliminating the problems related to moving adjacent heaps reversibly in a multi-heap layout. To prevent multiple, identical free-lists (e.g. free-lists pointing to same size blocks) occurring from having one free-list per record-type, we instead maintain free-lists per power-of-two.

The only drawback from this layout is the amount of internal fragmentation. As we only allocate blocks of a power-of-two size, substantial internal fragmentation follows when allocating large records, i.e. a allocating a block of size 128 for a record of size 65. However, as most real world programs uses much smaller sized records, we do not consider this a very frequent scenario.

Implementation-wise, this design would require doubling and halving of numbers related to the power-of-two. This action translates well into the reversible setting, as a simply bit-shifting directly gives us the desired result.

\lstinputlisting[caption={The Buddy Memory algorithm implemented in extended Janus.}, language=janus, style=basic, label={lst:buddy-memory}]{buddy-memory-report.ja}

Listing~\ref{lst:buddy-memory} shows the buddy memory algorithm implemented in the extended Janus variant with local blocks from~\cite{ty:ejanus}. For simplification in object sizes are rounded to the nearest power-of-two and we only allow allocations using the heads of the free lists.
The body of the allocation function is executed recursively until a free block larger or equal to the size of the object has been found. Once found, said block is popped from the free list. If the block is larger than the object we are allocating (rounded to nearest power-of-two), the block is split recursively until a block the desired size is obtained.

% TODO: Graphics of buddy memory layout
\begin{figure}[ht]
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \begin{tikzpicture}
      \draw (0,0) -- (1, 0);
    \end{tikzpicture}
    \caption{\footnotesize Initial memory block}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \begin{tikzpicture}
      \draw (0,0) -- (1, 0);
    \end{tikzpicture}
    \caption{\footnotesize Allocate an object of size $n$}
  \end{subfigure}%
  \vskip 1em
  \begin{subfigure}{.5\textwidth}
    \centering
    \begin{tikzpicture}
      \draw (0,0) -- (1, 0);
    \end{tikzpicture}
    \caption{\footnotesize Allocate an object of size $4n$}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \begin{tikzpicture}
      \draw (0,0) -- (1, 0);
    \end{tikzpicture}
    \caption{\footnotesize Allocate an object of size $2n$}
  \end{subfigure}%
  \caption{Buddy Memory block splitting for allocation}
  \label{fig:buddy-memory-block-splitting}
\end{figure}


% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.7\textwidth]{heap-designs}
%   \caption{Heap memory layouts (Draft)}
% \end{figure}

